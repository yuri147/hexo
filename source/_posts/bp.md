---
title: 超酷的反向传播算法
tags:
  - 神经网络
  - 机器学习
categories: 
  - Tech
---
## 人工智能
  目前最火的技术莫过于人工智能，或者说机器学习。从IBM Watson到Google AlphaGo,人工智能仿佛已经冲出了实验室，在实际生活中发挥作用。面对如此高大上的技术，普通老百姓要如何去看待它，理解它的本质呢?沉着冷静别惊慌，本文将尽可能给你一个答案。
  
  首先要知道机器学习的本质是算法，这里就会有好几种，比如：神经网络、支持向量机、朴素贝叶斯等等一堆。虽然算法很多，但他们主要解决的都是同一个问题---分类预测问题。为什么分类预测问题这么重要？因为机器学习的本质是重现人类学习的过程，这个过程可以大致分为两部分：1.定义一个事物，2.判断一个事物。如果一个机器可以对一个事物进行判断，判断的结果与人类的判断相似，那它近似的就是一个人工智能。

  假设这样一个场景，我和机器都看到了一个苹果，按照之前对机器的训练如果他告诉我他看到了一个苹果，那说明这个机器是具有一定人工智能的，如果它还能告诉我这个苹果的产地，成熟度，净重，品种、颜色、气味特征等等....我也不会惊讶，因为这是描述一个苹果的特征，也是机器定义这是一个苹果的依据。你看，这里就出现了一个人类与机器的差异，我们学习一个事物并不需要太多维度的特征来描述一个事物，比如，在某某地区生长的、重量在这个范围的、颜色可能是这种的、可能有这些形状的、...（此处省略无限字，因为可以从无限个维度去描述一个苹果）,OK,这个东西叫苹果！我们只要摸过吃过看过，大概就知道啥是苹果，也不会和梨搞错，为什么！因为我们聪明，没错，我们的大脑做了定义和判断的工作并且是在我们无意识的情况下。从这个角度上来说，其实研究机器学习的本质其实是研究人类自己认识这个世界的过程。  
   

## 神经网络
  好了，说点实际的。本篇文章的主题就是人工神经网络中的反向传播算法（Back Propagation Algorithm，BP算法）。反向传播算法是实现人工神经网络（Neural Networks，NNs）中非常重要的技术，就是它让神经网络变的“智能”。
  首先神经网络的模型是这个样子的，这是一个简化了的神经网络结构，图中球模拟了神经元的细胞，线模拟了神经元的突触，简而言之它在用数学模型模拟我们的大脑：

<center> <img src="/images/bp/bp1.svg"> </center>

  
  在左侧有i1,i2，表示输入层；最右侧有o1,o2,表示输出层。当我们在训练机器学习的时候，会把输入值和输出值都设定好，好比说，i1=0.15，i2=0.10；计算结果应该是o1=0.01,o2=0.99，如果训练是成功的，那当我输入相同的输入值时，结果应该也是相同的。是不是有点像我们在构造一个函数，而这个函数我们并不能看到，所以很多人说神经网络是一个黑盒模型。所以，我们需要在中间加入一层来描述其中转换的过程，由于是不可见的，这层叫隐含层h1,h2。现在我们需要初始化节点之间的连线，为这些连线加上随机的权值。这些初始化的权值会在之后的计算中被更新，事实上这些权值就是描述这个机器思考的模型。在计算的过程中，我们还会用到b1,b2，这称为偏置项，值永远是1，权值可以自由设置，这里我们设b1权值为0.35，b2权值为0.60。OK,现在这个模型变成了这样：
  
<center> <img src="/images/bp/bp2.svg"> </center>
  
### 前向传播
  好了，一切就绪，我们要开始算了！怎么算呢，分为2步，第1步是简单的加权相加或者叫做线性回归，第2步是代入一个激活函数，激活函数的作用是将线性函数表达为非线性函数，它会把值挤压进一个(0,1)区间的范围，同时可以反应出条件概率，激活函数可以使用sigmoid函数,exp函数表示了以e为底的指数函数:
  $$f(z)=\frac {1} {1+exp^{(-z)}}$$
```
function sigmoid(z){
  return 1 / (1 + Math.exp(-z));
}
```
  第1步，计算线性回归：
$$n_h1=i_1 × w_1 + i_2 × w_2 + b_1 × 1=0.15 × 0.1 + 0.1 × 0.2 + 0.35 × 1=0.385$$
```
 nH[0]=i[0] * w[0] + i[1] * w[1] + b[0] * 1;
```
  第2步，激活：
  $$out_h1=\frac {1} {1+exp^{(-n_h1)}}=\frac {1} {1+exp^{(-0.385)}}=0.595078473866134$$
```
 outH[0]=sigmoid(nH[0]);
```
  现在我们就计算出了h1节点的值，用相同的方法，我们计算出h2节点：
  <center>$out_h2=0.5980868603322034$</center>
  同样的我们也可以算出o1,o2:
  <center>
  $out_o1=0.7286638276265998$，$out_o2=0.751601224586807$  
  </center>
  可以看到计算的结果和我们设定的结果(0.01,0.99)有很大的误差，没关系，接下来我们需要减小这个误差。
### 计算总误差
  误差计算通过平方误差函数为每个节点计算误差,然后将这些误差相加计算总误差:
  $$E_total=\sum \frac 1 2(target - current )^2$$
  
<!--$$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$-->
<!--\\(x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\\)-->






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" async src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>